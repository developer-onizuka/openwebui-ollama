apiVersion: v1
kind: ConfigMap
metadata:
  name: model-config
  namespace: default
data:
  MODEL_FILENAME: "gpt-oss-20b-Q4_K_M.gguf"
  MODEL_DOWNLOAD_URL: "https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-Q4_K_M.gguf"
  #MODEL_FILENAME: "Llama-3.2-1B-Instruct-Q4_K_M.gguf"
  #MODEL_DOWNLOAD_URL: "https://huggingface.co/bartowski/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_K_M.gguf"
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: llama-cpp-python
  name: svc-llama-cpp-python
  namespace: default
spec:
  #type: LoadBalancer
  ports:
  - port: 8000
  selector:
    app: llama-cpp-python
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: llama-cpp-python
  name: llama-cpp-python
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: llama-cpp-python
  template:
    metadata:
      labels:
        app: llama-cpp-python
    spec:
      containers:
      - image: developeronizuka/llama-cpp-python:arm64
        name: llama-cpp-python
        ports:
        - containerPort: 8000
        command: ["/usr/bin/python3", "-m", "llama_cpp.server"]
        args:
        - "--model"
        - "/app/models/$(MODEL_FILENAME)"
        - "--host"
        - "0.0.0.0"
        - "--port"
        - "8000"
        - "--n_gpu_layers"
        - "0"
        - "--n_ctx"
        - "2048"
        env:
        - name: MODEL_FILENAME
          valueFrom:
            configMapKeyRef:
              name: model-config
              key: MODEL_FILENAME
        - name: MODEL_DOWNLOAD_URL
          valueFrom:
            configMapKeyRef:
              name: model-config
              key: MODEL_DOWNLOAD_URL
        volumeMounts:
        - name: llama-cpp-python-csi-data
          mountPath: /app/models
        resources:
          requests:
            memory: "6Gi"
            cpu: "3"
          limits:
            memory: "8Gi"
            cpu: "4"
      initContainers:
      - name: download-model
        image: busybox
        command:
        - sh
        - -c
        - |
          FILE_PATH="/app/models/$(MODEL_FILENAME)"
          if [ ! -f "$FILE_PATH" ]; then
            echo "Model file not found. Downloading..."
            wget -O "$FILE_PATH" "$(MODEL_DOWNLOAD_URL)"
          else
            echo "Model file already exists. Skipping download."
          fi
        volumeMounts:
        - name: llama-cpp-python-csi-data
          mountPath: /app/models
        env:
        - name: MODEL_FILENAME
          valueFrom:
            configMapKeyRef:
              name: model-config
              key: MODEL_FILENAME
        - name: MODEL_DOWNLOAD_URL
          valueFrom:
            configMapKeyRef:
              name: model-config
              key: MODEL_DOWNLOAD_URL
      volumes:
      - name: llama-cpp-python-csi-data
        persistentVolumeClaim:
          claimName: pvc-nfs-llama-cpp-python

