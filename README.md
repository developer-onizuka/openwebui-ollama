Open WebUI & Ollama

# 1. Goal
Hugging FaceのTransformersライブラリは、**LLM**の学習や推論を行う上で非常に強力なツールです。特に、pipelineメソッドはモデルの推論を簡潔に実行できるため、多くの開発者に利用されています。<br>
しかし、Transformersの実装には、LLMの推論時にいくつかの非効率性が課題として挙げられています。これらの課題は、推論速度やGPUメモリの利用効率に影響を与え、特に大規模なモデルやリアルタイムでの利用においてボトルネックとなることがあります。

**Transformersを使った例**：<br>
> https://github.com/developer-onizuka/MachineLearningOnAWS/blob/main/BERT_FineTuning.ipynb <br>

この課題を解決するために、vLLMやOllamaといったライブラリが登場しました。これらは独自のアルゴリズムを採用し、特にKey-Value (KV) Cacheの割り当てと管理を最適化することで、推論の非効率性を大幅に改善しています。<br><br>
**vLLM**は、PagedAttentionと呼ばれる革新的なアルゴリズムを導入し、TransformerのKV Cacheをページングされたメモリシステムとして扱います。これにより、メモリの断片化（フラグメンテーション）が解消され、メモリの無駄を大幅に削減しつつ、高いスループットを実現しています。<br>
**Ollama**もまた、KV Cacheの効率的な管理を目的としたアルゴリズムを採用しています。これにより、キャッシュのメモリ無駄を大幅に削減するとともに、高速なアクセスと更新を実現し、ローカル環境でのLLM実行をより手軽に行えるようにしています。<br>

### 各推論フレームワークの比較

| 項目 | Hugging Face Transformers | vLLM | Ollama |
|---|---|---|---|
| **主な目的** | LLMの学習・推論・開発の汎用的なフレームワーク。 | 高スループット・低遅延なLLM推論を可能にするサーバー。 | ローカル環境でのLLM実行・管理を簡単にすること。 |
| **KVキャッシュ管理** | 従来の方式（非効率性が課題）。 | **PagedAttention**による効率的なメモリ管理。 | KVキャッシュを最適化し、メモリの無駄を削減。 |
| **パフォーマンス** | 標準的な推論速度。単一の推論には適しているが、高負荷時のスループットは低い。 | 従来の方式と比べて、非常に高いスループットと低遅延を実現。 | ローカル環境では十分な性能を発揮するが、vLLMほどの高スループットは期待できない。 |
| **ハードウェア要件** | GPU、CPUなど幅広い環境で動作。 | NVIDIA GPU (CUDA) に最適化されている。 | GPU（NVIDIA、Apple Siliconなど）、CPUに対応。幅広い環境で動作。 |
| **使いやすさ** | Pythonライブラリとして提供され、柔軟性が高い。`pipeline`メソッドで推論は簡単。 | Pythonライブラリとして提供。OpenAI互換APIも提供され、サーバーとして構築しやすい。 | 非常にシンプルで、コマンド一つでモデルのダウンロードや実行が可能。 |
| **主なユースケース** | LLMの学習、モデル開発、実験、研究。 | 多数のユーザーに対応する本番環境の推論サーバー、リアルタイムアプリケーション。 | 個人でのモデル検証、ローカルでの開発、データプライバシーが重要な用途。 |

<br>
ここでは、Ollamaの推論フレームワークをKubernetes上で動作させ、Open WebUIと連携させることで、生成AIに直感的で使いやすいWeb UIを付加し、手軽に利用できる環境を構築することを目指すものです。


# 2. Kunernetes環境について
# 2-1. 各ノード
| Node名 | CPU | Memory | IP Address |
|---|---|---|---|
| master | 4 | 8GB | 192.168.33.100 |
| worker1 | 4 | 8GB | 192.168.33.101 |
| nfs | 1 | 1GB | 192.168.33.11 |

# 2-2. ロードバランサー

# 3. 手順









